{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae927b81",
   "metadata": {},
   "source": [
    "# TLC Data Prep & Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89cbd5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3\n",
    "import geopandas as gpd\n",
    "import pandas as pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc9dfd",
   "metadata": {},
   "source": [
    "[Data Source](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "> Download data: https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip.    \n",
    "> Unzip the file and store under uberProj/data/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f139f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read shapefile\n",
    "zones_gdf = gpd.read_file(\"../data/taxi_zones/taxi_zones.shp\")\n",
    "zones_gdf = zones_gdf.to_crs(epsg=4326) # convert to WGS84 from Web Mercator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57daf025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "      <th>h3_list</th>\n",
       "      <th>distribution_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>POLYGON ((-74.18445 40.695, -74.18449 40.6951,...</td>\n",
       "      <td>[882a1071edfffff, 882a1071e7fffff, 882a1071e5f...</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>MULTIPOLYGON (((-73.82338 40.63899, -73.82277 ...</td>\n",
       "      <td>[882a10395dfffff, 882a103955fffff, 882a103951f...</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.084341</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>POLYGON ((-73.84793 40.87134, -73.84725 40.870...</td>\n",
       "      <td>[882a100103fffff, 882a10011dfffff, 882a100119f...</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((-73.97177 40.72582, -73.97179 40.725...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.092146</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>POLYGON ((-74.17422 40.56257, -74.17349 40.562...</td>\n",
       "      <td>[882a106017fffff, 882a1060e1fffff, 882a1060e5f...</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID  Shape_Leng  Shape_Area                     zone  LocationID  \\\n",
       "0         1    0.116357    0.000782           Newark Airport           1   \n",
       "1         2    0.433470    0.004866              Jamaica Bay           2   \n",
       "2         3    0.084341    0.000314  Allerton/Pelham Gardens           3   \n",
       "3         4    0.043567    0.000112            Alphabet City           4   \n",
       "4         5    0.092146    0.000498            Arden Heights           5   \n",
       "\n",
       "         borough                                           geometry  \\\n",
       "0            EWR  POLYGON ((-74.18445 40.695, -74.18449 40.6951,...   \n",
       "1         Queens  MULTIPOLYGON (((-73.82338 40.63899, -73.82277 ...   \n",
       "2          Bronx  POLYGON ((-73.84793 40.87134, -73.84725 40.870...   \n",
       "3      Manhattan  POLYGON ((-73.97177 40.72582, -73.97179 40.725...   \n",
       "4  Staten Island  POLYGON ((-74.17422 40.56257, -74.17349 40.562...   \n",
       "\n",
       "                                             h3_list  distribution_factor  \n",
       "0  [882a1071edfffff, 882a1071e7fffff, 882a1071e5f...             0.100000  \n",
       "1  [882a10395dfffff, 882a103955fffff, 882a103951f...             0.055556  \n",
       "2  [882a100103fffff, 882a10011dfffff, 882a100119f...             0.250000  \n",
       "3                                                 []             0.000000  \n",
       "4  [882a106017fffff, 882a1060e1fffff, 882a1060e5f...             0.125000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to hexes based on geo\n",
    "def get_hexes(geo, res=8):\n",
    "    try:\n",
    "        return h3.geo_to_cells(geo, res)\n",
    "    except Exception as e:\n",
    "        print(\"Error in get_hexes:\", e)\n",
    "        return set()\n",
    "\n",
    "zones_gdf['h3_list'] = zones_gdf.geometry.apply(lambda x: get_hexes(x))\n",
    "zones_gdf['distribution_factor'] = zones_gdf['h3_list'].apply(\n",
    "    lambda x: 1.0 / len(x) if len(x) > 0 else 0\n",
    ")\n",
    "\n",
    "zones_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9283d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1070 unique H3 cells (full coverage)\n",
      "Saved zone lookups to ../data/zone_h3_lookup_polyfill.parquet\n"
     ]
    }
   ],
   "source": [
    "# Explode to create lookup table\n",
    "zone_lookup = zones_gdf[['LocationID', 'h3_list', 'distribution_factor']].explode('h3_list')\n",
    "zone_lookup = zone_lookup.rename(columns={'h3_list': 'h3_index'}).dropna()\n",
    "\n",
    "print(f\"{zone_lookup['h3_index'].nunique()} unique H3 cells (full coverage)\")\n",
    "\n",
    "# Save\n",
    "zone_lookup.to_parquet('../data/zone_h3_lookup_polyfill.parquet', index=False)\n",
    "print(f\"Saved zone lookups to ../data/zone_h3_lookup_polyfill.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14281309",
   "metadata": {},
   "source": [
    "## Zone Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38198342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zone lookup shape: (1070, 3)\n",
      "Unique H3 cells: 1070\n",
      "\n",
      "Sample of zone lookup:\n",
      "   LocationID         h3_index  distribution_factor\n",
      "0           1  882a1071edfffff                  0.1\n",
      "1           1  882a1071e7fffff                  0.1\n",
      "2           1  882a1071e5fffff                  0.1\n",
      "3           1  882a1071adfffff                  0.1\n",
      "4           1  882a1071e3fffff                  0.1\n",
      "\n",
      "Running query ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837baffbef7341ca839d03567f2c299a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESULTS ---\n",
      "Total records: 11,619,229\n",
      "Date range: 2022-01-01 01:00:00 to 2025-11-01 00:00:00\n",
      "Unique H3 cells: 1070\n",
      "\n",
      "Traffic statistics:\n",
      "count    1.161923e+07\n",
      "mean     1.135127e+01\n",
      "std      3.777377e+01\n",
      "min      3.448276e-02\n",
      "25%      2.000000e-01\n",
      "50%      5.000000e-01\n",
      "75%      2.800000e+00\n",
      "max      1.239000e+03\n",
      "Name: traffic_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Load the polyfill lookup\n",
    "zone_lookup = pd.read_parquet('../data/zone_h3_lookup_polyfill.parquet')\n",
    "\n",
    "print(f\"Zone lookup shape: {zone_lookup.shape}\")\n",
    "print(f\"Unique H3 cells: {zone_lookup['h3_index'].nunique()}\")\n",
    "print(\"\\nSample of zone lookup:\")\n",
    "print(zone_lookup.head())\n",
    "\n",
    "# Connect to DuckDB and register the lookup\n",
    "con = duckdb.connect()\n",
    "con.register('zone_lookup', zone_lookup)\n",
    "\n",
    "# Build file list\n",
    "files = []\n",
    "for year in range(2022, 2025):\n",
    "    for month in range(1, 13):\n",
    "        files.append(\n",
    "            f\"'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{month:02d}.parquet'\"\n",
    "        )\n",
    "\n",
    "# 2025: only Jan-Oct\n",
    "for month in range(1, 11):\n",
    "    files.append(\n",
    "        f\"'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-{month:02d}.parquet'\"\n",
    "    )\n",
    "\n",
    "files_list = f\"[{', '.join(files)}]\"\n",
    "\n",
    "# 4. QUERY for distribution (using duckdb enhanced speed by ~50x)\n",
    "query = f\"\"\"\n",
    "WITH trips_with_zones AS (\n",
    "    SELECT\n",
    "        t.tpep_pickup_datetime,\n",
    "        z.h3_index,\n",
    "        z.distribution_factor\n",
    "    FROM read_parquet({files_list}) t\n",
    "    JOIN zone_lookup z ON t.PULocationID = z.LocationID\n",
    "    WHERE t.tpep_pickup_datetime >= '2022-01-01' \n",
    "      AND t.tpep_pickup_datetime < '2025-11-01'\n",
    "),\n",
    "hourly_agg AS (\n",
    "    SELECT\n",
    "        h3_index,\n",
    "        date_trunc('hour', tpep_pickup_datetime) AS hour_bin,\n",
    "        -- IMPORTANT: Use SUM(distribution_factor) to distribute traffic\n",
    "        SUM(distribution_factor) AS traffic_count\n",
    "    FROM trips_with_zones\n",
    "    GROUP BY h3_index, hour_bin\n",
    ")\n",
    "SELECT\n",
    "    h3_index,\n",
    "    hour_bin + INTERVAL '1 hour' AS match_hour,\n",
    "    traffic_count\n",
    "FROM hourly_agg\n",
    "ORDER BY h3_index, match_hour;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nRunning query ...\")\n",
    "traffic_polyfill = con.execute(query).fetchdf()\n",
    "\n",
    "print(f\"\\n--- RESULTS ---\")\n",
    "print(f\"Total records: {len(traffic_polyfill):,}\")\n",
    "print(f\"Date range: {traffic_polyfill['match_hour'].min()} to {traffic_polyfill['match_hour'].max()}\")\n",
    "print(f\"Unique H3 cells: {traffic_polyfill['h3_index'].nunique()}\")\n",
    "print(f\"\\nTraffic statistics:\")\n",
    "print(traffic_polyfill['traffic_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d973e4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- COVERAGE ANALYSIS ---\n",
      "Collision cells: 1135\n",
      "Traffic cells: 1070\n",
      "Overlapping cells: 997 (87.8%)\n",
      "Collision cells with ZERO traffic: 138 (12.2%)\n"
     ]
    }
   ],
   "source": [
    "# Compare with collision data\n",
    "collision_cells = set(pd.read_csv('../data/h3_full_panel_res8.csv')['h3_index'].unique())\n",
    "traffic_cells = set(traffic_polyfill['h3_index'].unique())\n",
    "\n",
    "overlap = collision_cells & traffic_cells\n",
    "missing = collision_cells - traffic_cells\n",
    "\n",
    "print(f\"\\n--- COVERAGE ANALYSIS ---\")\n",
    "print(f\"Collision cells: {len(collision_cells)}\")\n",
    "print(f\"Traffic cells: {len(traffic_cells)}\")\n",
    "print(f\"Overlapping cells: {len(overlap)} ({len(overlap)/len(collision_cells)*100:.1f}%)\")\n",
    "print(f\"Collision cells with ZERO traffic: {len(missing)} ({len(missing)/len(collision_cells)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebbcd6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After cleaning: 11,619,229 records\n",
      "✓ Saved to ../data/traffic_h3_2022_2025_polyfill.parquet\n"
     ]
    }
   ],
   "source": [
    "# Filter to valid date range (clean any bad dates)\n",
    "traffic_clean = traffic_polyfill[\n",
    "    (traffic_polyfill['match_hour'].dt.year >= 2022) & \n",
    "    (traffic_polyfill['match_hour'].dt.year <= 2025)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nAfter cleaning: {len(traffic_clean):,} records\")\n",
    "\n",
    "# Save the traffic data\n",
    "traffic_clean.to_parquet('../data/traffic_h3_2022_2025_polyfill.parquet', index=False)\n",
    "print(\"✓ Saved to ../data/traffic_h3_2022_2025_polyfill.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f944eb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing cells: ['882a10750dfffff', '882a1001dbfffff', '882a10776bfffff', '882a10019dfffff', '882a106267fffff', '882a1072dbfffff', '882a1076d3fffff', '882a107213fffff', '882a10742dfffff', '882a103827fffff']\n",
      "\n",
      "Crashes in missing cells: 10,275\n",
      "% of total crashes: 3.02%\n"
     ]
    }
   ],
   "source": [
    "# Missing cells & Crashes in missing cells\n",
    "\n",
    "collision_df = pd.read_csv('../data/h3_full_panel_res8.csv')\n",
    "traffic_df = pd.read_parquet('../data/traffic_h3_2022_2025_polyfill.parquet')\n",
    "\n",
    "collision_cells = set(collision_df['h3_index'].unique())\n",
    "traffic_cells = set(traffic_df['h3_index'].unique())\n",
    "missing_cells = collision_cells - traffic_cells\n",
    "\n",
    "print(f\"Missing cells: {list(missing_cells)[:10]}\")\n",
    "\n",
    "# Check how many crashes are in these missing cells\n",
    "missing_collisions = collision_df[collision_df['h3_index'].isin(missing_cells)]\n",
    "print(f\"\\nCrashes in missing cells: {missing_collisions['accidents_count'].sum():,}\")\n",
    "print(f\"% of total crashes: {missing_collisions['accidents_count'].sum() / collision_df['accidents_count'].sum() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae7d77c",
   "metadata": {},
   "source": [
    "## Validation & Integration Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc6fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "✓ Traffic data: 11,619,229 rows, 1070 H3 cells\n",
      "✓ Collision panel: 38,871,480 rows, 1135 H3 cells\n",
      "✓ Weather data: 34,248 rows\n"
     ]
    }
   ],
   "source": [
    "# 1. Load all datasets\n",
    "print(\"Loading datasets...\")\n",
    "weather_df = pd.read_csv('../data/nyc_weather_hourly.csv')\n",
    "\n",
    "print(f\"\\u2713 Traffic data: {len(traffic_df):,} rows, {traffic_df['h3_index'].nunique()} H3 cells\")\n",
    "print(f\"\\u2713 Collision panel: {len(collision_df):,} rows, {collision_df['h3_index'].nunique()} H3 cells\")\n",
    "print(f\"\\u2713 Weather data: {len(weather_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b1092ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAFFIC DATA VALIDATION ===\n",
      "Date range: 2022-01-01 01:00:00 to 2025-11-01 00:00:00\n",
      "\n",
      "Null values:\n",
      "h3_index         0\n",
      "match_hour       0\n",
      "traffic_count    0\n",
      "dtype: int64\n",
      "\n",
      "Traffic count statistics:\n",
      "count    1.161923e+07\n",
      "mean     1.135127e+01\n",
      "std      3.777377e+01\n",
      "min      3.448276e-02\n",
      "25%      2.000000e-01\n",
      "50%      5.000000e-01\n",
      "75%      2.800000e+00\n",
      "max      1.239000e+03\n",
      "Name: traffic_count, dtype: float64\n",
      "\n",
      "Rows with zero/negative traffic: 0 (0.00%)\n",
      "✓ Traffic data integrity: PASSED\n"
     ]
    }
   ],
   "source": [
    "# 2. Validate traffic data integrity\n",
    "print(\"\\n=== TRAFFIC DATA VALIDATION ===\")\n",
    "\n",
    "# Date range\n",
    "traffic_df['match_hour'] = pd.to_datetime(traffic_df['match_hour'])\n",
    "print(f\"Date range: {traffic_df['match_hour'].min()} to {traffic_df['match_hour'].max()}\")\n",
    "\n",
    "# Check for nulls\n",
    "nulls = traffic_df.isnull().sum()\n",
    "print(f\"\\nNull values:\\n{nulls}\")\n",
    "assert nulls.sum() == 0, \"Traffic data has null values!\"\n",
    "\n",
    "# Traffic statistics\n",
    "print(f\"\\nTraffic count statistics:\")\n",
    "print(traffic_df['traffic_count'].describe())\n",
    "\n",
    "# Check for negative or zero traffic\n",
    "invalid = traffic_df[traffic_df['traffic_count'] <= 0]\n",
    "print(f\"\\nRows with zero/negative traffic: {len(invalid):,} ({len(invalid)/len(traffic_df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\u2713 Traffic data integrity: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edec1204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COVERAGE ANALYSIS ===\n",
      "Collision H3 cells: 1,135\n",
      "Traffic H3 cells: 1,070\n",
      "Overlapping cells: 997 (87.8%)\n",
      "Collision cells WITHOUT traffic: 138 (12.2%)\n",
      "Traffic cells NOT in collisions: 73\n",
      "\n",
      "Crashes in cells WITHOUT traffic: 10,275 / 339,749 (3.02%)\n",
      "Sample missing cells: ['882a10750dfffff', '882a1001dbfffff', '882a10776bfffff', '882a10019dfffff', '882a106267fffff']\n",
      "✓ Coverage analysis: COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# 3. Coverage analysis: collision cells vs traffic cells\n",
    "print(\"\\n=== COVERAGE ANALYSIS ===\")\n",
    "\n",
    "collision_cells = set(collision_df['h3_index'].unique())\n",
    "traffic_cells = set(traffic_df['h3_index'].unique())\n",
    "\n",
    "overlap = collision_cells & traffic_cells\n",
    "missing_traffic = collision_cells - traffic_cells\n",
    "extra_traffic = traffic_cells - collision_cells\n",
    "\n",
    "print(f\"Collision H3 cells: {len(collision_cells):,}\")\n",
    "print(f\"Traffic H3 cells: {len(traffic_cells):,}\")\n",
    "print(f\"Overlapping cells: {len(overlap):,} ({len(overlap)/len(collision_cells)*100:.1f}%)\")\n",
    "print(f\"Collision cells WITHOUT traffic: {len(missing_traffic):,} ({len(missing_traffic)/len(collision_cells)*100:.1f}%)\")\n",
    "print(f\"Traffic cells NOT in collisions: {len(extra_traffic):,}\")\n",
    "\n",
    "# Check how many crashes occur in cells with no traffic data\n",
    "if len(missing_traffic) > 0:\n",
    "    missing_collisions = collision_df[collision_df['h3_index'].isin(missing_traffic)]\n",
    "    total_crashes = collision_df['accidents_count'].sum()\n",
    "    missing_crashes = missing_collisions['accidents_count'].sum()\n",
    "    print(f\"\\nCrashes in cells WITHOUT traffic: {missing_crashes:,} / {total_crashes:,} ({missing_crashes/total_crashes*100:.2f}%)\")\n",
    "    print(f\"Sample missing cells: {list(missing_traffic)[:5]}\")\n",
    "\n",
    "print(\"\\u2713 Coverage analysis: COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a17c1747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEMPORAL ALIGNMENT ===\n",
      "Collision range: 2022-01-01 00:00:00 to 2025-11-27 23:00:00\n",
      "Traffic range: 2022-01-01 01:00:00 to 2025-11-01 00:00:00\n",
      "Weather range: 2022-01-01 00:00:00 to 2025-11-27 23:00:00\n",
      "\n",
      "Common overlap: 2022-01-01 01:00:00 to 2025-11-01 00:00:00\n",
      "Overlap duration: 1399 days\n",
      "✓ Temporal alignment: PASSED\n"
     ]
    }
   ],
   "source": [
    "# 4. Temporal alignment test\n",
    "print(\"\\n=== TEMPORAL ALIGNMENT ===\")\n",
    "\n",
    "# Normalize datetime columns\n",
    "collision_df['date'] = pd.to_datetime(collision_df['date'])\n",
    "collision_df['datetime'] = collision_df['date'] + pd.to_timedelta(collision_df['hour'], unit='h')\n",
    "\n",
    "weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "weather_df['datetime'] = weather_df['date'] + pd.to_timedelta(weather_df['hour'], unit='h')\n",
    "\n",
    "# Date ranges\n",
    "print(f\"Collision range: {collision_df['datetime'].min()} to {collision_df['datetime'].max()}\")\n",
    "print(f\"Traffic range: {traffic_df['match_hour'].min()} to {traffic_df['match_hour'].max()}\")\n",
    "print(f\"Weather range: {weather_df['datetime'].min()} to {weather_df['datetime'].max()}\")\n",
    "\n",
    "# Find overlap window\n",
    "collision_start, collision_end = collision_df['datetime'].min(), collision_df['datetime'].max()\n",
    "traffic_start, traffic_end = traffic_df['match_hour'].min(), traffic_df['match_hour'].max()\n",
    "weather_start, weather_end = weather_df['datetime'].min(), weather_df['datetime'].max()\n",
    "\n",
    "overlap_start = max(collision_start, traffic_start, weather_start)\n",
    "overlap_end = min(collision_end, traffic_end, weather_end)\n",
    "\n",
    "print(f\"\\nCommon overlap: {overlap_start} to {overlap_end}\")\n",
    "print(f\"Overlap duration: {(overlap_end - overlap_start).days} days\")\n",
    "\n",
    "assert overlap_start < overlap_end, \"No temporal overlap between datasets!\"\n",
    "print(\"\\u2713 Temporal alignment: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5197c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SAMPLE MERGE TEST ===\n",
      "Sample cell: 882a10002bfffff\n",
      "Sample week: 2022-01-01 to 2022-01-08\n",
      "\n",
      "Sample data sizes:\n",
      "  Collision rows: 168\n",
      "  Traffic rows: 1\n",
      "  Weather rows: 168\n",
      "\n",
      "Merged rows: 168\n",
      "Traffic matched: 1 / 168\n",
      "\n",
      "Sample merged data:\n",
      "          h3_index            datetime  accidents_count  traffic_count  \\\n",
      "0  882a10002bfffff 2022-01-01 01:00:00                0            NaN   \n",
      "1  882a10002bfffff 2022-01-01 02:00:00                0            NaN   \n",
      "2  882a10002bfffff 2022-01-01 03:00:00                0            NaN   \n",
      "3  882a10002bfffff 2022-01-01 04:00:00                0            NaN   \n",
      "4  882a10002bfffff 2022-01-01 05:00:00                0            NaN   \n",
      "\n",
      "   rain_flag  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          1  \n",
      "✓ Sample merge: PASSED\n"
     ]
    }
   ],
   "source": [
    "# 5. Sample merge test (spot-check integration)\n",
    "print(\"\\n=== SAMPLE MERGE TEST ===\")\n",
    "\n",
    "# Take a sample H3 cell from the overlap\n",
    "sample_cell = list(overlap)[0]\n",
    "sample_start = overlap_start\n",
    "sample_end = sample_start + pd.Timedelta(days=7)\n",
    "\n",
    "print(f\"Sample cell: {sample_cell}\")\n",
    "print(f\"Sample week: {sample_start.date()} to {sample_end.date()}\")\n",
    "\n",
    "# Extract sample data\n",
    "collision_sample = collision_df[\n",
    "    (collision_df['h3_index'] == sample_cell) &\n",
    "    (collision_df['datetime'] >= sample_start) &\n",
    "    (collision_df['datetime'] < sample_end)\n",
    "]\n",
    "\n",
    "traffic_sample = traffic_df[\n",
    "    (traffic_df['h3_index'] == sample_cell) &\n",
    "    (traffic_df['match_hour'] >= sample_start) &\n",
    "    (traffic_df['match_hour'] < sample_end)\n",
    "]\n",
    "\n",
    "weather_sample = weather_df[\n",
    "    (weather_df['datetime'] >= sample_start) &\n",
    "    (weather_df['datetime'] < sample_end)\n",
    "]\n",
    "\n",
    "print(f\"\\nSample data sizes:\")\n",
    "print(f\"  Collision rows: {len(collision_sample)}\")\n",
    "print(f\"  Traffic rows: {len(traffic_sample)}\")\n",
    "print(f\"  Weather rows: {len(weather_sample)}\")\n",
    "\n",
    "# Attempt merge on (h3, datetime)\n",
    "merged = collision_sample.merge(\n",
    "    traffic_sample, \n",
    "    left_on=['h3_index', 'datetime'], \n",
    "    right_on=['h3_index', 'match_hour'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged rows: {len(merged)}\")\n",
    "print(f\"Traffic matched: {merged['traffic_count'].notna().sum()} / {len(merged)}\")\n",
    "\n",
    "if len(merged) > 0:\n",
    "    print(f\"\\nSample merged data:\")\n",
    "    print(merged[['h3_index', 'datetime', 'accidents_count', 'traffic_count', 'rain_flag']].head())\n",
    "\n",
    "print(\"\\u2713 Sample merge: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7aa8e839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INTEGRATION READINESS SUMMARY\n",
      "============================================================\n",
      "✓ PASS Traffic data integrity\n",
      "✓ PASS Coverage overlap\n",
      "✓ PASS Temporal alignment\n",
      "✓ PASS Merge compatibility\n",
      "\n",
      "✅ ALL CHECKS PASSED - Ready for integration!\n",
      "Ready for causal inference with real traffic data!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6. Integration readiness summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTEGRATION READINESS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checks = {\n",
    "    \"Traffic data integrity\": True,\n",
    "    \"Coverage overlap\": len(overlap) / len(collision_cells) > 0.8,\n",
    "    \"Temporal alignment\": (overlap_end - overlap_start).days > 365,\n",
    "    \"Merge compatibility\": len(merged) > 0\n",
    "}\n",
    "\n",
    "for check, passed in checks.items():\n",
    "    status = \"\\u2713 PASS\" if passed else \"\\u2717 FAIL\"\n",
    "    print(f\"{status} {check}\")\n",
    "\n",
    "if all(checks.values()):\n",
    "    print(\"\\n\\u2705 ALL CHECKS PASSED - Ready for integration!\")\n",
    "    print(f\"Ready for causal inference with real traffic data!\")\n",
    "else:\n",
    "    print(\"\\n\\u26a0\\ufe0f SOME CHECKS FAILED - Review before integration\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
